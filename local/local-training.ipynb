{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, we need to install the relevant libaries required for this local training usecase. You will need to pip install boto3, pandas and Sagemaker into your Python environment. For this lab, running Python 3.9 as your kernel is recommended. Please see the requirements.txt for what's needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker.predictor import csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "from causalnex.discretiser import Discretiser\n",
    "import warnings\n",
    "from causalnex.structure import StructureModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from causalnex.network import BayesianNetwork\n",
    "from causalnex.evaluation import classification_report\n",
    "from causalnex.evaluation import roc_auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a SageMaker Local session. Please insert the ARN for Sagemeker execution role below. \n",
    "\n",
    "*Note*\n",
    "\n",
    "*You will need to create an IAM role in your AWS account and ensure it has permissions to SageMaker, S3 and ECR.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = LocalSession()\n",
    "sagemaker_session.config = {'local': {'local_code': True}}\n",
    "\n",
    "role = 'arn:aws:iam::403775705461:role/SageMaker-IAM-Role-AB3'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the data location, which is apart of this repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = \"./data/heart_failure_clinical_records_dataset.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "In the cell, we are doing data pre-processing to make our dataset ML friendly. This code is using a Discretiser class from the causalnex library to transform a continuous feature into a discrete one. \n",
    "\n",
    "Taking age as an example, we are using an numeric_split_points=[60], which means it will split the data into two bins: below 60 and above 60. Similar approaches are used on other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalnex.discretiser import Discretiser\n",
    "import pandas as pd\n",
    "\n",
    "initial_df = pd.read_csv(data_location)\n",
    "\n",
    "initial_df[\"age\"] = Discretiser(method=\"fixed\", numeric_split_points=[60]).transform(\n",
    "    initial_df[\"age\"].values\n",
    ")\n",
    "initial_df[\"serum_sodium\"] = Discretiser(method=\"fixed\", numeric_split_points=[136]).transform(\n",
    "    initial_df[\"serum_sodium\"].values\n",
    ")\n",
    "initial_df[\"serum_creatinine\"] = Discretiser(\n",
    "    method=\"fixed\", numeric_split_points=[1.1, 1.4]\n",
    ").transform(initial_df[\"serum_sodium\"].values)\n",
    "\n",
    "initial_df[\"ejection_fraction\"] = Discretiser(\n",
    "    method=\"fixed\", numeric_split_points=[30, 38, 42]\n",
    ").transform(initial_df[\"ejection_fraction\"].values)\n",
    "\n",
    "initial_df[\"creatinine_phosphokinase\"] = Discretiser(\n",
    "    method=\"fixed\", numeric_split_points=[120, 540, 670]\n",
    ").transform(initial_df[\"creatinine_phosphokinase\"].values)\n",
    "\n",
    "initial_df[\"platelets\"] = Discretiser(method=\"fixed\", numeric_split_points=[263358]).transform(\n",
    "    initial_df[\"platelets\"].values\n",
    ")\n",
    "\n",
    "print (\"Dataset after pre-processing\")\n",
    "initial_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "In the next cell, we will be training the model on our dataset from above. You should see training information when the training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = StructureModel()\n",
    "sm.add_edges_from([\n",
    "    ('ejection_fraction', 'DEATH_EVENT'),\n",
    "    ('creatinine_phosphokinase', 'DEATH_EVENT'),\n",
    "    ('age','DEATH_EVENT'),\n",
    "    ('smoking','high_blood_pressure'),\n",
    "    ('age','high_blood_pressure'),            \n",
    "    ('serum_sodium','DEATH_EVENT'),\n",
    "    ('high_blood_pressure','DEATH_EVENT'),\n",
    "    ('anaemia','DEATH_EVENT'),\n",
    "    ('creatinine_phosphokinase','DEATH_EVENT'),\n",
    "    ('smoking','DEATH_EVENT')\n",
    "])\n",
    "\n",
    "train, test = train_test_split(initial_df, train_size=0.8, test_size=0.2, random_state=42)\n",
    "        \n",
    "bn = BayesianNetwork(sm)\n",
    "bn = bn.fit_node_states(initial_df)\n",
    "bn = bn.fit_cpds(train, method=\"BayesianEstimator\", bayes_prior=\"K2\")\n",
    "\n",
    "roc, auc = roc_auc(bn, test, \"DEATH_EVENT\")\n",
    "print(\"Model AUC: \" + str(auc))\n",
    "\n",
    "print(classification_report(bn, test, \"DEATH_EVENT\"))\n",
    "\n",
    "# save the model\n",
    "model_path = \"models\"\n",
    "isExist = os.path.exists(model_path)\n",
    "if not isExist:\n",
    "   os.makedirs(model_path)\n",
    "with open(os.path.join(model_path, 'causal_model.pkl'), 'wb') as out:\n",
    "    pickle.dump(bn, out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Docker Container Locally\n",
    "\n",
    "For this step, we need to create a docker image in our local environment. Please ensure you have 'sagemaker-causalnex-local' in your Docker Images. \n",
    "\n",
    "1. Please ensure Docker is running locally\n",
    "2. Please run 'docker build -t sagemaker-causal-nex:latest .' in the 'sagemaker-local-to-cloud/local/container' path\n",
    "3. From the previous steps, please insert the IAM role from your AWS account with permissions to access SageMaker, S3 and ECR and insert the ARN in the 'role_value' section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to change to ../local/container/Dockerfile when finished\n",
    "! docker build -f ../local/container_SM_Toolkit/Dockerfile -t sagemaker-causal-nex:latest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.local import LocalSession\n",
    "\n",
    "image = 'sagemaker-causal-nex'\n",
    "\n",
    "env={\n",
    "    \"MODEL_SERVER_WORKERS\":\"2\"\n",
    "    }\n",
    "\n",
    "local_regressor = Estimator(\n",
    "    image,\n",
    "    role = 'arn:aws:iam::403775705461:role/SageMaker-IAM-Role-AB3',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"local\")\n",
    "\n",
    "train_location = 'file://'+data_location\n",
    "\n",
    "local_regressor.fit(train_location, logs=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can launch this container and run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = local_regressor.deploy(1, 'local', env=env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We can now send a sample JSON payload for inference with our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = open('payload.json')\n",
    "test_data1 = '{\"age\": 1, \"anaemia\": 0, \"creatinine_phosphokinase\": 2, \"diabetes\": 0, \"ejection_fraction\": 0, \"high_blood_pressure\": 1, \"platelets\": 1, \"serum_creatinine\": 0, \"serum_sodium\": 0, \"sex\": 1, \"smoking\": 0, \"time\": 4}'\n",
    "\n",
    "#with open('payload.json') as f:\n",
    "#    d = json.load(f)\n",
    "#    s = json.dumps(d)\n",
    "#    print(d)\n",
    "\n",
    "with open('payload.json') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "\n",
    "# s = json.dumps(test_data1)\n",
    "print (test)\n",
    "print(type(test)) \n",
    "# print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predictor.predict(test[\"data\"]).decode('utf-8')\n",
    "# predicted = predictor.predict(test[\"data\"]).decode('utf-8')\n",
    "\n",
    "# predicted = predictor.predict(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the Container to ECR on the AWS Cloud\n",
    "\n",
    "At this Point, we have successfully launched the container on our local machine and we are able to send inference commands. We would now like to push this container to the ECR repository on the AWS Cloud.  \n",
    "\n",
    "We start by defining some variables like the current execution role, the ECR repository that we are going to use for pushing the custom Docker container and a default Amazon S3 bucket to be used by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "ecr_namespace = \"sagemaker-local-training-containers/\"\n",
    "prefix = \"local-training\"\n",
    "\n",
    "ecr_repository_name = ecr_namespace + prefix\n",
    "account_id = role.split(\":\")[4]\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(account_id)\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the Dockerfile which defines the statements for building our custom SageMaker training container:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize ../local/container/Dockerfile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At high-level the Dockerfile specifies the following operations for building this container:\n",
    "\n",
    "TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and push the container\n",
    "We are now ready to build this container and push it to Amazon ECR. It will create a new repo in ECR for you. Please ensure you have the correct IAM permissions to push to the ECR.\n",
    "\n",
    "In the below cell we are building, tagging, authenticating and pushing the container to ECR with the above variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker build -f ../local/container/Dockerfile -t sagemaker-local-training-containers/tutorial ../local/container\n",
    "! docker tag sagemaker-local-training-containers/tutorial {account_id}.dkr.ecr.{region}.amazonaws.com/sagemaker-local-training-containers/local-training:latest\n",
    "! aws ecr get-login --no-include-email --registry-ids {account_id}\n",
    "! aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com\n",
    "! aws ecr describe-repositories --repository-names sagemaker-local-training-containers/local-training || aws ecr create-repository --repository-name sagemaker-local-training-containers/local-training\n",
    "! docker push {account_id}.dkr.ecr.{region}.amazonaws.com/sagemaker-local-training-containers/local-training:latest\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternaitvely you can use the below cell and leverage the python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# ! ../scripts/build_and_push.sh $account_id $region $ecr_repository_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down the endpoint we created for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charm-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
